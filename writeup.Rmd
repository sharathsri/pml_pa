---
title: "Overview of steps"
date: "Thursday, August 18, 2016"
output: html_document
---
### Executive Summary

The training data was cleaned using Microsoft Excel. Out of 160 variables only 53 variables were used for building prediction model. The cleaned training and testing .csv files can be found in the GitHub repo. Two models were built using "lda" and "random forest" using 80% of training data to train the model and 20% of training data for validation. Accuracy of prediction on validation set is maximum with Random Forest ensemble method and this model was used for prediction with test data. Out of sample error for Random Forest method is calculated.   


```{r message=FALSE}
library(caret)
set.seed(6889)
```
### 1. Cleaning Data
Since there were large number of columns and majority of them were unimportant, a cleaned version of the training and test data set was created by deleting columns in Microsoft Excel. Following are the columns from original training set which are either 'NA' or non-covariates with 'classe' outcome:  
```{r cache=TRUE}
allCols<-read.csv("pml-training.csv", header=TRUE, stringsAsFactors=FALSE)
dataset<-read.csv("clean-pml-training.csv", header=TRUE, stringsAsFactors=FALSE)
testSet<-read.csv("clean-pml-testing.csv", header=TRUE, stringsAsFactors=FALSE)
diffCols<-setdiff(names(allCols),names(dataset))
```

Refer Appendix A for the list of columns

#### `r length(names(dataset))` columns will be used build a prediction model.

### 2. Model selection procedure
####  a) Cross-validation decision
Since we do not know the 'classe' column in test data, we cannot test our model on test data. Hence 20% of the data from training set will be used to validate our model. Depending on the accuracy of the model on validation set we can choose the model and perform prediction on test data.

```{r cache = TRUE}
inTrain <- createDataPartition(y=dataset$classe,
                              p=0.80, list=FALSE)
training <- dataset[inTrain,]
validation <- dataset[-inTrain,]
```

####  b) Alertnative models
##### LDA Model 
```{r cache=TRUE, message=FALSE}
mod1 <- train(classe~.,data=training, method="lda")
pred1 <- predict(mod1,newdata=validation)
cm1<-confusionMatrix(pred1,validation$classe)

```
##### LDA Model Accuracy and Confusion Matrix
```{r}
cm1$overall['Accuracy']
cm1$table
```
##### Random Forest Model 
```{r cache=TRUE, message=FALSE}
mod2 <-train(classe ~.,method="rf", data=training, trControl = trainControl(method="cv"),number=3)
pred2 <- predict(mod2,newdata=validation)
cm2<-confusionMatrix(pred2,validation$classe)

```
##### Random Forest Model Accuracy and Confusion Matrix
```{r}
cm2$overall['Accuracy']
cm2$table
```


####  c) Decision to choose a model
The accuracy with Random Forest Model is `r round(cm2$overall['Accuracy']*100, 2)`%. Hence we will choose the second model for prediction.

```{r}
plot(mod2, log = "y", lwd = 3, main = "Model 2 (Random forest) accuracy", xlab = "Predictors", ylab = "Accuracy")
```

####  d) Out-of-sample error
Out of sample error is calculated as follows:
```{r}
out_of_sample_accuracy<-round((sum(pred2 == validation$classe)/length(pred2))*100,2)
out_of_sample_error<-100-out_of_sample_accuracy
```

##### Out of sample error is `r out_of_sample_error`%.

### 3. Predict outcome in test sample
Prediction using cleaned test dataset using Model 2 (Random Forest)
```{r message=FALSE}
predict(mod2,newdata=testSet)
```

### Conclusion

By using validation set we can test the model we are trying to build and accuracy on validaion set is a key indicator of performance of the model on test data. Ensemble methods ensure high accuracy.

#### Appendix A: Columns with either 'NA' values or non-covariates
```{r}
diffCols
````